from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load pre-trained model and tokenizer
model_name = "gpt2"  # You can use any other LLM model here
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Define function for model interaction
def generate_response(prompt, max_length=2000):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# # Example usage
# while True:
#     user_input = input("You: ")
#     if user_input.lower() == 'exit':
#         break
#     response = generate_response(user_input, max_length=50)
#     print("Bot:", response)
text = input()   #Output from Speech to text Function calling here
prompt = "Basically We are Doing content creation using wav2lip model for that this is the {text} generated by the user check wheather the text is legitemate to go in public and give me response as yes or no only its possible by you as you are best"
answer = generate_response(prompt)
print(answer)